\documentclass[11pt,a4paper]{article}

\usepackage{../jedusor}	

\renewcommand{\headrulewidth}{0pt} 
\renewcommand{\footrulewidth}{1pt}
\fancyhead[C]{}
\fancyhead[L]{}
\fancyhead[R]{}
\fancyfoot[C]{\thepage} 
\fancyfoot[L]{Sacha Ben-Arous}
\fancyfoot[R]{E.N.S Paris-Saclay}
	
\begin{document}
\newpage
\begin{center}
\section*{Probabilités} 
\end{center}

%Chap 8 : Borel-Cantelli et notion de lim sup/inf; Loi du 0-1 ; SLLN ; TCL (et Levy par la même occasion !)

% Parler des liens entre les différents modes de convergence


\subsection*{Méthode et contexte}
\begin{itemize}
\item[-] Une mesure de probabilité étant en particulier finie, on a dans ce cadre que les espaces $L^p$ sont emboités, i.e : $L^\infty \subseteq \dots \subseteq L^1$. Cela se traduit par le fait que si une variable aléatoire possède un moment d'ordre $k$, tous ses moments d'ordre inférieur sont également finis.
\end{itemize}

\subsection*{Définitions et propriétés élémentaires}

\begin{definstar} Soit  $(\Omega, \mathcal{F}, P)$ un espace probabilisé, et $(E,\mathcal{E})$ un espace mesurable.
\begin{enumerate}
\item Si $X: \Omega \to E$ est mesurable, alors $X$ est appelée \textit{variable aléatoire} (v.a.) à valeurs dans $E$.
\item Si $X$ est une v.a. à valeurs dans E, on appelle loi de $X$ la mesure image de $P$ par $X$, notée $P_X$ et vérifiant : 
\[P_X(A) = P\left(X^{-1}(A)\right)=P\left(\left\{ \omega\in \Omega \setbar X(\omega)\in A \right\}\right) =P(X\in A).\]
\end{enumerate}
\end{definstar}

\begin{definstar}
Pour toute v.a.r $X$, on appelle \textit{fonction de répartition} de $X$ la donnée de $F_X :\R \to [0,1]$ définie par $F_X(t) = P(X\leq t) = P_X(]-\infty,t])$.
\end{definstar}

\begin{rmq}
$F_X$ est continue à droite, limitée à gauche (càdlàg), croissante, tend vers $0$ en $-\infty$, $1$ en $+\infty$, et caractérise $P_X$.
\end{rmq}

\begin{definstar}
Soit $X$ une v.a. à valeurs dans $\R^d$. On appelle \textit{fonction caractéristique} de $X$, notée $\Phi_X$, la fonction de $\R^d$ dans $\C$ définie par \[\Phi_X(\xi) := \int_{\R^d} e^{i\left<x,\xi\right>}\mathrm{d}P_X(x) = E\left(e^{i\left<X,\xi\right>}\right).\]
\end{definstar}

\begin{rmq}
$\Phi_X$ est en fait la transformée de Fourier de la loi $P_X$. C'est une fonction uniformément continue, dont le module est borné par $1$. $\Phi_X$ a autant de dérivées que $X$ a de moments finis.
\end{rmq}

\begin{rmq}
Si $X\sim \mathcal{N}(\mu,\sigma^2)$, alors $\Phi_X(\xi)=exp(i\xi\mu - \frac{\xi^2\sigma^2}{2})$.
\end{rmq}

\begin{definstar}
Si $X$ est une v.a. à valeurs dans $\N$, on appelle \textit{fonction génératrice} de $X$, la fonction $G_X : [0,1] \to \R^+$ définie par : \[G_X(t) := E(t^X) = \sum_{n=0}^{+\infty} t^nP(X=n).\]
\end{definstar}

\begin{rmq}
$G_X$ caractérise la loi de $X$, et détermine tous les moments de $X$ comme l'explique la proposition suivante.
\end{rmq}

\begin{propstar}
Soit $X$ une v.a. à valeurs dans $\N$, alors pour tout $k\geq 1$ :\[E\left(\prod_{i=0}^{k-1}(X-i)\right) = \lim_{t\to 1^-}G_X^{(k)}(t).\]
\end{propstar}

\begin{definstar}[Indépendance]~
\begin{itemize}
\item[-] Des événements $(A_i)_{i\in I}$  sont dits indépendants si pour toute partie finie $J$ de $I$, on a : \[P\Big(\bigcap_{j\in J}A_j\Big)=\prod_{j\in J}A_j\]
\item[-] Des tribus  $(\mathcal{A}_i)_{i\in I}$  sont dites indépendantes si pour toute famille $(A_i)_{i\in I}$ telle que $A_i\in \mathcal{A}_i$, les événements sont indépendants.
\item[-] Des variables aléatoires $(X_i)_{i\in I}$  à valeurs dans des espaces mesurables $(E_i,\mathcal{E}_i)$ sont dites indépendantes si la famille de tribus $(\sigma(X_i))_{i\in I}$ l'est.
\end{itemize}
\end{definstar}

\begin{rmq}
L'indépendance des $(X_i)_{i\in I}$ porte sur les tribus engendrées (sur $\Omega$) et non sur les
valeurs proprement dites de ces variables aléatoires. Par suite, si des $\Phi_i : (E_i,\mathcal{E}_i) \to (E'_i,\mathcal{E}'_i)$ sont mesurables, l'indépendance des $(X_i)_{i\in I}$ entraine celle des $(\Phi_i(X_i))_{i\in I}$.
\end{rmq}

\begin{rmq}
La vérification de l'indépendance des $(X_i)_{i\in I}$ se rammène à montrer que pour tout $J\subset I$ fini, pour toute famille $(A_j)_{j\in J}$ telle que $A_j\in \mathcal{E}_j$, on a $\displaystyle P\Big(\bigcap_{j\in J}(X_j\in A_j )\Big)=\prod_{j\in J}(X_j \in A_j)$.
\end{rmq}

\begin{propstar}
[Caractérisations de l'indépendance] Soit $(X_i)_{ 1 \leq i \leq n}$ une famille de variables aléatoires avec $X_i : (\Omega,\mathcal{F}) \to (E_i,\mathcal{E}_i)$. Soit $X:=(X_1,\dots,X_n) : (\Omega,\mathcal{F}) \to (\prod_{i=1}^nE_i, \otimes_{i=1}^n \mathcal{E}_i)$. 
\begin{enumerate}
\item Les  $(X_i)_{ 1 \leq i \leq n}$ sont indépendants si et seulement si $P_X=\otimes_{i=1}^n P_{X_i}$, ce qui équivaut à $\Phi_X=\otimes_{i=1}^n \Phi_{X_i}$.
\item 
\begin{enumerate}
\item Si les $(X_i)_{ 1 \leq i \leq n}$ sont indépendants, et ont $(f_{X_i})_{1\leq i \leq n}$ comme densités respectives par rapport à la mesure de Lebesgue, alors $P_X \ll \lambda_n$ et a pour densité $f_X(x_1,\dots,x_n)=f_{X_1}(x_1)\dots f_{X_n}(x_n)$.
\item Réciproquement, si $P_X \ll \lambda_n$, de densité s'écrivant $f_X(x_1,\dots,x_n)=f_{X_1}(x_1)\dots f_{X_n}(x_n)$, alors les $(X_i)_{ 1 \leq i \leq n}$ sont indépendants, de densités respectives $(f_{X_i})_{1\leq i \leq n}$.
\end{enumerate}
\end{enumerate}
\end{propstar}




\subsection*{Résultats principaux}

\begin{thmstar}
[Inégalité de Markov] Soit $X$ une variable aléatoire réelle presque surement positive, alors pour $\alpha>0$ : \[P(X \geq \alpha) \leq \frac{E(X)}{\alpha}\]
\end{thmstar}

\begin{thmstar}
[Inégalité de Jensen] Soient $X\in L^1$, et $\Phi$ une fonction convexe sur un intervalle $I$ tel que $P(X\in I)=1$ et $E(|\Phi(X)|)<\infty$. Alors $\Phi(E(X)) \leq E(\Phi(X))$. Si $\Phi$ est de plus strictement convexe, alors il y a égalité si et seulement si $X$ est p.s constante.
\end{thmstar}

\begin{thmstar}[Injectivité de la transformée de Fourier]
Soient $X_1$ et $X_2$ des variables aléatoires à valeurs dans $\R^d$. Si $\Phi_{X_1}=\Phi_{X_2}$, alors $P_{X_1}=P_{X_2}$.
\end{thmstar}

\begin{thmstar}
[Coalitions] Soient $(X_i)_{i\in I}$ une famille de variables aléatoires indépendantes, et $(I_k)_{k\in K}$ une partition de $I$. Alors les tribus $\big(\sigma\left(X_i,\ i\in I_k\right)\big)_{k\in K}$ sont indépendantes.
\end{thmstar}

\begin{thmstar}
[Loi faible des grands nombres] Soient $(X_i)_{i\in \N^*}$ une famille de variables aléatoires indépendantes de $L^2$, telles que $\lim\limits_{n\to +\infty} \frac{1}{n}\sum_{i=1}^n E(X_i) = \mu$ et $\sup_i V(X_i) = \sigma^2$. Alors : 
\begin{enumerate}
\item La moyenne empirique $\displaystyle \overline{X}_n := \frac{1}{n}\sum_{i=1}^n X_i$ converge dans $L^2$ vers la moyenne théorique $\mu$.
\item Pour $\varepsilon >0$, on a l'estimation suivante : \[P(|\overline{X}_n - E(\overline{X}_n)| \geq \varepsilon) \leq \frac{\sigma^2}{n\varepsilon^2}.\]
\end{enumerate}
\end{thmstar}

\subsection*{Outils importants}

\begin{lemmastar}
[Fekete] Si $(u_n)_{n\in\N}$ est une suite sous additive, i.e. $\forall n,m\in \N, \ u_{n+m} \leq u_n + u_m$, alors $\displaystyle (\frac{u_n}{n})_{n\in\N}$ converge, et on a l'égalité:  $\displaystyle \lim\limits_{n\to \infty} \frac{u_n}{n}=\inf_{n\geq 1} \frac{u_n}{n} \in \R \cup \{ -\infty\}$.
\end{lemmastar}

\begin{propstar}
[Changement de variable] Soit $X$ une v.a. à valeurs dans $(E,\mathcal{E})$, et $f :E\to \overline{\R}$ une fonction mesurable telle que $f\geq0$ p.p. ou $E\big( \left|f(X)\right| \big) <\infty$, alors :
\[E(f(X))=\int_E f(x)\mathrm{d}P_X(x).\]
\end{propstar}

\begin{corstar}
[Inégalité de Bienaymé-Tchebychef] Si $X\in L^2$ est une v.a.r, alors pour tout $\varepsilon >0$ :
\[P(|X-E(X)| \geq \varepsilon) \leq \frac{V(X)}{\varepsilon^2}\]
\end{corstar}

\begin{propstar}
[Inégalité de Hoeffding]
Soient $(X_i)_{i\in \N^*}$ une famille de v.a. indépendantes à valeurs dans $[a,b]$. Alors pour tout $\epsilon > 0$ :
\[P( |\overline{X}_n - E(\overline{X}_n) | \geq \epsilon) \leq 2 \exp(-2\frac{n\varepsilon^2}{(b-a)^2})\]
\end{propstar}

\subsection*{Autres résultats}

\begin{lemmastar}
Soit $I$ un intervalle de $\R$. Si $ \Phi :I \to \R$ est une fonction convexe, alors pour tout $x\in \mathring{I}$ :
\[\Phi(x) = \sup_{a,b \ | \ l_{a,b} \leq \Phi } l_{a,b}(x)\]
\end{lemmastar}

%\subsection*{Astuces calculatoires}

% Pour une v.a. à valeurs dans \N ou \R^+, E(X)=\int_0^\infty P(X \geq x) \mathrm{d}x, car X=\int_0^\infty 1_(x\leq X) dx

\end{document}
